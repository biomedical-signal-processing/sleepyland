{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring Services via the `manager-api`\n",
    "\n",
    "---\n",
    "\n",
    "Links to notebooks in this repository:\n",
    "\n",
    "[Quickstart Tutorial](./quickstart_tutorial.ipynb) | [Introduction](./00_introduction.ipynb) | [Services](./01_services.ipynb) | [Sleep Staging](./02_sleep_staging.ipynb) | [Ensembling Sleep Staging](./03_ensembling_sleep_staging.ipynb) | [Sleep Dynamics](./04_sleep_dynamics.ipynb) | [Luna Toolbox Integration](./05_luna_integration.ipynb)\n",
    "\n",
    "---"
   ],
   "id": "22d5c12982dc78cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this section, we will guide you through the various services available in SLEEPYLAND. However, please note that we will not cover how to access or call the `gui` service and the `notebook` service directly, as the `gui` is already running in the background with the main purpose of easily expose the SLEEPYLAND tool, and the `notebook` is actually the interface you are currently using to interact with all other services.\n",
    "\n",
    "\n",
    "\n",
    "> ## *Accessing the `gui` Service*\n",
    "> Even though you are using the Jupyter Notebook to interact with SLEEPYLAND, you can still access the `gui` when needed. This service provides a web-based interface for interacting with the various functionalities of SLEEPYLAND. \n",
    "> \n",
    "> To open the GUI service, follow these steps:\n",
    "> 1. Ensure that all containers are running. You can check the status of all your Docker containers by executing the following command in your terminal:\n",
    "> \n",
    "> ```bash\n",
    "> docker ps\n",
    "> ```\n",
    "> \n",
    "> This command lists all the running containers along with their names and status. Look for the container named gui in the output to confirm that the `gui` service is up.\n",
    "> \n",
    "> 2. Open your web browser and navigate to the following URL:\n",
    ">```plaintext\n",
    "> http://localhost:8887\n",
    "> ```\n",
    ">\n",
    "> This will take you to the SLEEPYLAND GUI, where you can perform various tasks interactively.\n",
    "> \n",
    "> By using the Jupyter Notebook, you can efficiently manage and interact with other services while the GUI runs in the background for additional functionalities.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "33188a88db4ad618"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> ## *Helper function for making POST requests*\n",
    "> Below a reusable helper function for making POST requests and performing specific tasks. The function defined here allow you to interact with the different services via the `manager-api` for different operations such as downloading data, harmonizing datasets, retrieving channels, making predictions.\n",
    "\n",
    "> Important Note: Make sure all Docker containers are running before you attempt to send requests to the `manager-api`."
   ],
   "id": "2c4bbba198b9a06"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "# Define the base URL for the manager-api\n",
    "MANAGER_API_BASE_URL = \"http://manager-api:8989\"\n",
    "\n",
    "def make_post_request(endpoint, data=None, params=None):\n",
    "    \"\"\"\n",
    "    Helper function to make a POST request to the specified endpoint.\n",
    "\n",
    "    Parameters:\n",
    "        endpoint (str): The API endpoint to hit.\n",
    "        data (dict, optional): The form data to send in the request.\n",
    "        params (dict, optional): The URL parameters to send in the request.\n",
    "\n",
    "    Returns:\n",
    "        dict: The JSON response if the request is successful.\n",
    "    \"\"\"\n",
    "    url = f\"{MANAGER_API_BASE_URL}/{endpoint}\"\n",
    "    response = requests.post(url, data=data, params=params)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Success:\", response.json())\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed with status code {response.status_code}\")\n",
    "        return None"
   ],
   "id": "8b86b74ec5522c1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Accessing the `nsrr-download` Service\n",
    "\n",
    "---\n",
    "\n",
    "The `nsrr-download` service is responsible for downloading sleep data from the National Sleep Research Resource (NSRR). It interacts with the underlying file system to manage the input files, execute download commands, and handle dataset organization. \n",
    "\n",
    "In this section, we will explain how to access and use the `nsrr-download` service via the `manager-api`. \n",
    "\n",
    "The `nsrr-download` service exposes the following endpoint:\n",
    "\n",
    "- **`POST /download_data`**: This endpoint allows you to download data by providing a token and a list of selected recordings/data.\n",
    "\n",
    "### Key Functionalities of the `nsrr-download` Service\n",
    "\n",
    "1. **Download Data**:\n",
    "   - The service accepts a list of recordings/data in the format `edf_path+ann_path`, where each data consists of an EDF file and its corresponding XML annotation file.\n",
    "   - It uses the `download_data` function to initiate the download process for each specified data.\n",
    "\n",
    "2. **Command Execution**:\n",
    "   - The service runs a shell script (`nsrr_download.sh`) to handle the actual downloading of files.\n",
    "   - It utilizes the `subprocess` module to execute shell commands securely.\n",
    "\n",
    "3. **File Management**:\n",
    "   - After downloading, the service creates an `input` folder for each EDF file, moves the downloaded files into the appropriate directory, and performs cleanup to remove temporary files or directories.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - The service provides error handling to ensure that any issues encountered during the download process are logged and communicated back to the user."
   ],
   "id": "f4e6bc0dedd9b535"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To interact with the `nsrr-download` service, you'll use the `manager-api`, which handles requests and forwards them to the appropriate service. Hereâ€™s how you can do it:",
   "id": "a8c406ca376152a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This function sends a request to download specific datasets (EDF and annotation files) from the manager-api.\n",
    "# It uses the user's token for authentication and specifies the paths of the datasets to be downloaded.\n",
    "def download_data(edf_path, annotation_path, token):\n",
    "    # Combine the edf and annotation paths\n",
    "    data_path = edf_path + \"+\" + annotation_path\n",
    "    \n",
    "    # Specify the datasets to download\n",
    "    datasets = [data_path]\n",
    "    \n",
    "    data = {'token': token, 'selected_datasets': datasets}\n",
    "    make_post_request(\"download_data\", data=data)"
   ],
   "id": "7cb88e70926a7873"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Replace 'your_token_here' with your actual NSRR token\n",
    "token = 'your_token_here'\n",
    "\n",
    "# Specify the edf and annotation paths for the data you want to download\n",
    "edf_path = \"abc/polysomnography/edfs/baseline/abc-baseline-900001.edf\"\n",
    "annotation_path = \"abc/polysomnography/annotations-events-nsrr/baseline/abc-baseline-900001-nsrr.xml\"\n",
    "\n",
    "# Use the download_data function to download the specified EDF and annotation files.\n",
    "download_data(edf_path, annotation_path, token)"
   ],
   "id": "cbaea4c1d3a0b265"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Accessing the `wild-to-fancy` service\n",
    "\n",
    "---\n",
    "\n",
    "The `wild-to-fancy` service is responsible for harmonizing PSG datasets, converting them into a format suitable for analysis. This service interacts with uploaded datasets and processes them according to specified commands.\n",
    "\n",
    "In this section, we will explain how to access and use the `wild-to-fancy` service via the `manager-api`.\n",
    "\n",
    "The `wild-to-fancy` service exposes the following endpoint:\n",
    "\n",
    "- **`POST /harmonize`**: This endpoint allows you to harmonize uploaded datasets by providing the folder name and dataset type.\n",
    "\n",
    "### Key Functionalities of the `wild-to-fancy` Service\n",
    "\n",
    "1. **Harmonize Datasets**:\n",
    "   - The service processes the uploaded datasets, specifically `.edf` and `.xml` files, according to predefined command templates specified in JSON configuration files.\n",
    "   - It organizes the harmonized output files in the appropriate directory structure.\n",
    "\n",
    "2. **Command Execution**:\n",
    "   - The service runs specific shell commands to perform the harmonization process, ensuring that each command is executed in the correct context.\n",
    "\n",
    "3. **File Management**:\n",
    "   - After processing, the service cleans up temporary directories and removes the original uploaded files to maintain a tidy workspace.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - The service provides error handling to ensure that any issues encountered during the harmonization process are logged and communicated back to the user.\n",
    "\n"
   ],
   "id": "48ab551e47944270"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To interact with the `wild-to-fancy` (i.e., harmonizer) service, you'll use the `manager-api` to send a request to harmonize the datasets. Hereâ€™s how to do it:\n",
    "\n",
    "> Before sending a request, ensure that your dataset is uploaded and structured properly (check the `input` folder). The uploaded dataset should include the necessary `.edf` and `.xml` files."
   ],
   "id": "eeae4599351ffaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This function sends a request to harmonize the specified dataset, which ensures that the dataset conforms to a standard format.\n",
    "def harmonize_data(dataset):\n",
    "    data = {'dataset': dataset}\n",
    "    make_post_request(\"harmonize\", data=data)"
   ],
   "id": "59c1c6ddd59e0633"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = 'abc' \n",
    "\n",
    "# Use the harmonize_data function to harmonize the dataset for consistency.\n",
    "harmonize_data(dataset)"
   ],
   "id": "ec537dd410e854b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Accessing the `usleepyland` service\n",
    "\n",
    "---\n",
    "The `usleepyland` service is responsible for processing and generating evaluations/predictions based on uploaded PSG datasets. It is designed to operate on uploaded datasets, which are processed based on specified commands and configurations.\n",
    "\n",
    "In this section, we will explain how to access and use the usleepyland service via the manager-api.\n",
    "\n",
    "The `usleepyland` service exposes the following endpoints:\n",
    "\n",
    "- **`GET /get_channels`**: This endpoint allows you to retrieve the channel information for each dataset. It is useful for understanding the available channels in the dataset.\n",
    "- **`POST /predict`**: This endpoint allows you to generate predictions from uploaded PSG datasets. It requires some parameters to initiate the prediction process.\n",
    "\n",
    "### Key Functionalities of the `usleepyland` Service\n",
    "\n",
    "1. **Retrieve Channel Information**:\n",
    "   - The service provides information about the channels available in the uploaded datasets.\n",
    "   - It helps users understand the data structure and select the appropriate channels for prediction.\n",
    "2. **Prediction Generation**:\n",
    "   - The service generates predictions based on the processed datasets and models selected for the prediction task.\n",
    "   - It provides the prediction results in a structured format for further analysis and interpretation."
   ],
   "id": "adcc3833270e46e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This function sends a request to retrieve the available channels for a given dataset.\n",
    "# Channels could include EEG AND/OR EOG AND/OR EMG derivations depending on the dataset.\n",
    "def get_channels(dataset):\n",
    "    params = {'dataset': dataset}\n",
    "    make_post_request(\"get_channels\", params=params)"
   ],
   "id": "cf8b0687db009b08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the get_channels function to retrieve the available channels for the dataset.\n",
    "get_channels(dataset)"
   ],
   "id": "178905f5880c5a7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After selecting the channels for prediction, you can send a request to the `usleepyland` service to generate predictions. Use the following code snippet to initiate the prediction process.\n",
    "\n",
    "Explanation of the parameters:\n",
    "- `folder_root_name`: The root folder name where the input files are stored.\n",
    "- `output_folder_name`: The folder name for the output files.\n",
    "- `eeg_channels`: A list of EEG channels to use for prediction.\n",
    "- `eog_channels`: A list of EOG channels to use for prediction.\n",
    "- `emg_channels`: A list of EMG channels to use for prediction.\n",
    "- `dataset`: The name of the NSRR dataset to predict."
   ],
   "id": "e8ae624560345ba6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This function sends a request to perform a prediction task using the specified EEG, EOG, and EMG channels for the given dataset.\n",
    "# The results are stored in the specified output folder.\n",
    "def predict_data(folder_root_name, output_folder_name, eeg_channels, eog_channels, emg_channels, dataset, models):\n",
    "    data = {\n",
    "        'folder_root_name': folder_root_name,\n",
    "        'folder_name': output_folder_name,\n",
    "        'eeg_channels': eeg_channels,\n",
    "        'eog_channels': eog_channels,\n",
    "        'emg_channels': emg_channels,\n",
    "        'dataset': dataset,\n",
    "        'models': models\n",
    "    }\n",
    "    make_post_request(\"predict\", data=data)"
   ],
   "id": "3944be70f5df8a57"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the channels and dataset details for the sleep staing prediction\n",
    "\n",
    "eeg_channels = ['F3-M2,F4-M1,C3-M2,C4-M1,O1-M2,O2-M1']\n",
    "eog_channels = ['E1-M2,E2-M1']\n",
    "emg_channels = ['']\n",
    "\n",
    "dataset = 'abc'\n",
    "\n",
    "models = ['yasa,usleep']\n",
    "\n",
    "# Use the predict_data function to run the prediction task on the dataset using specified channels.\n",
    "predict_data('abc', 'output_abc', eeg_channels, eog_channels, emg_channels, dataset, models)"
   ],
   "id": "4673c81d5c859214",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Accessing multiple services at once\n",
    "\n",
    "Below we demonstrate how to exploit the `manager-api` by calling two distinct services â€” `wild-to-fancy` and `usleepyland` â€” in a single command. The `auto_predict_data` function simplifies the process by combining these steps into one streamlined command. By using `auto_predict_data`, you can harmonize the dataset to ensure it meets the required formats and then immediately perform predictions on the same dataset."
   ],
   "id": "57d84a1719c77f54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_predict_data(folder_root_name, output_folder_name, eeg_channels, eog_channels, emg_channels, dataset, models):\n",
    "    data = {\n",
    "        'folder_root_name': folder_root_name,\n",
    "        'folder_name': output_folder_name,\n",
    "        'eeg_channels': eeg_channels,\n",
    "        'eog_channels': eog_channels,\n",
    "        'emg_channels': emg_channels,\n",
    "        'dataset': dataset,\n",
    "        'models': models\n",
    "    }\n",
    "    make_post_request(\"auto_predict\", data=data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the channels and dataset details for the sleep staing prediction\n",
    "\n",
    "eeg_channels = ['F3-M2,F4-M1,C3-M2,C4-M1,O1-M2,O2-M1']\n",
    "eog_channels = ['E1-M2,E2-M1']\n",
    "emg_channels = ['']\n",
    "\n",
    "dataset= 'abc'\n",
    "\n",
    "models = ['yasa,usleep']\n",
    "\n",
    "# Auto Predict Data\n",
    "# Use the auto_predict_data function to perform both harmonization and prediction in one step.\n",
    "auto_predict_data('abc', 'output_abc', eeg_channels, eog_channels, emg_channels, dataset, models)"
   ],
   "id": "8a9f6c44cdb63be2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
